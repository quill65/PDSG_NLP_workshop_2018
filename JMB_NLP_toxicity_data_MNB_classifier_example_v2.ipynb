{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# NLP pre-processing and classification using Multinomial Naive-Bayes \n",
    "\n",
    "This notebook is intended as an example of how to load, pre-process and classify the toxicity scored text comment data provided for the 2018 Portland Data Science Group NLP workshop. The code here uses a Multinomial Naive-Bayes classifier, which is commonly used for NLP classification tasks (though it is by no means the only one!). Additionally, I explore the effect of adjusting the number of training samples of each class (toxic vs non-toxic) on classifer accuracy.\n",
    "\n",
    "Setup:\n",
    "- Load data downloaded from http://dive-into.info/ - The data is expected to be in the same folder as this notebook.\n",
    "- Download and install nltk content (optional)\n",
    "- Combine comment and toxicity score data and generate toxicity categories (toxic vs non-toxic) for classifier training and prediction.\n",
    "\n",
    "Text pre-processing:\n",
    "- Clean up text by dropping non-alpha characters.\n",
    "- Drop words < 3 chars.\n",
    "- Use a word stemmer to stem the words.\n",
    "\n",
    "Classifier training:\n",
    "- Use TfidfVectorizer to create word count vectors, and then apply TDF/IDF algorithm to weight word vectors.\n",
    "- Fit (train) MultinomialNB classifier with vectorized word data. \n",
    "- Test classifier.\n",
    "- RUn Kfolds cross-validation to test robustness of the classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the notebook plot environment, import some basic modules, and load the data.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- This block also does a minimal bit of cleanup, by removing the embedded text \"NEWLINE_TOKEN\" and \"TAB_TOKEN\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments.shape =  (159686, 7)\n",
      "ratings.shape =  (1598289, 4)\n"
     ]
    }
   ],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "# set matplotlib environment and import some basics\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 100 # set to -1 to see entire text\n",
    "\n",
    "# ******************************************************\n",
    "# load the wikipedia toxicity data provided by Matt\n",
    "# ******************************************************\n",
    "# set True to load the smaller data set, False to load the large data set\n",
    "# NOTE: this code assumes data files are in the same folder as the notebook.\n",
    "if False:\n",
    "    # comment filename\n",
    "    commentfile = 'toxicity_annotated_comments_unanimous.tsv'\n",
    "    # rating filename\n",
    "    ratingfile = 'toxicity_annotations_unanimous.tsv'\n",
    "\n",
    "# full data set\n",
    "else:\n",
    "    # comment filename\n",
    "    commentfile = 'toxicity_annotated_comments.tsv'\n",
    "    # rating filename\n",
    "    ratingfile = 'toxicity_annotations.tsv'\n",
    "\n",
    "# load annotated comments\n",
    "comments = pd.read_table(commentfile)\n",
    "ratings = pd.read_table(ratingfile)\n",
    "\n",
    "# remove weird tab/newline TOKEN text\n",
    "comments['comment'] = comments['comment'].str.replace('NEWLINE_TOKEN','\\n')\n",
    "comments['comment'] = comments['comment'].str.replace('TAB_TOKEN','')\n",
    "\n",
    "# show shape of each data set\n",
    "print(\"comments.shape = \",comments.shape)\n",
    "print(\"ratings.shape = \",ratings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import NLTK components\n",
    "\n",
    "NLTK is installed in Jupyter by default, but you still need to download the corpus data many of the tools use to process text data. This only needs to be done once. I've commented out the code line that does this, so the first time you run this code, you may need to uncomment it and run this cell, then comment again since you only need to do it once. NLTK will open a dialog allowing you to select what is downloaded - I chose all, but probably \"most popular\" will suffice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# uncomment this to download nltk. This needs to be done once, and takes a while. \n",
    "#  I downloaded everything, but probably the \"popular packages\" will suffice.\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine comments and scores into one dataset\n",
    "\n",
    "Use Pandas groupby function to calculate the mean and median for each comment, and add them as columns to the comment dataframe. Now I have comments and two measures of score aligned.\n",
    "\n",
    "Next, I create a new toxicity categorical variable (0=not toxic, 1=toxic) by thresholding the median score at 0. I use the median score here because it is less sensitive to outlier scores than the mean.\n",
    "\n",
    "Note that I don't use the mean or median scores beyond this point - the Naive Bayes classifier wants a categorical variable. However, you could potentially do some other interesting things with these scores, including implement a different classifier that makes use of score data - just sayin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoredcomments.shape =  (159686, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>median_score</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232</td>\n",
       "      <td>This:\\n:One can make an analogy in mathematical terms by envisioning the distribution of opinion...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216</td>\n",
       "      <td>`\\n\\n:Clarification for you  (and Zundark's right, i should have checked the Wikipedia bugs page...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953</td>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547</td>\n",
       "      <td>`This is such a fun entry.   Devotchka\\n\\nI once had a coworker from Korea and not only couldn't...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959</td>\n",
       "      <td>Please relate the ozone hole to increases in cancer, and provide figures. Otherwise, this articl...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id  \\\n",
       "0    2232   \n",
       "1    4216   \n",
       "2    8953   \n",
       "3   26547   \n",
       "4   28959   \n",
       "\n",
       "                                                                                               comment  \\\n",
       "0  This:\\n:One can make an analogy in mathematical terms by envisioning the distribution of opinion...   \n",
       "1  `\\n\\n:Clarification for you  (and Zundark's right, i should have checked the Wikipedia bugs page...   \n",
       "2                                                                            Elected or Electoral? JHK   \n",
       "3  `This is such a fun entry.   Devotchka\\n\\nI once had a coworker from Korea and not only couldn't...   \n",
       "4  Please relate the ozone hole to increases in cancer, and provide figures. Otherwise, this articl...   \n",
       "\n",
       "   year  logged_in       ns  sample  split  mean_score  median_score  toxicity  \n",
       "0  2002       True  article  random  train         0.4           0.5         0  \n",
       "1  2002       True     user  random  train         0.5           0.0         0  \n",
       "2  2002      False  article  random   test         0.1           0.0         0  \n",
       "3  2002       True  article  random  train         0.6           0.0         0  \n",
       "4  2002       True  article  random   test         0.2           0.0         0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoredcomments = comments.copy()\n",
    "# group all scores by comment ID for each text sample, add mean and median score columns to comment data \n",
    "scoredcomments[\"mean_score\"] = pd.Series(ratings.groupby(\"rev_id\",as_index=False).mean()[\"toxicity_score\"])\n",
    "scoredcomments[\"median_score\"] = pd.Series(ratings.groupby(\"rev_id\",as_index=False).median()[\"toxicity_score\"])\n",
    "\n",
    "# create catgorical variable toxicity: if median score < 0, toxicity=1, otherwise 0\n",
    "scoredcomments[\"toxicity\"] = (scoredcomments[\"median_score\"] < 0).astype(int)\n",
    "\n",
    "# make the comment id s ints\n",
    "scoredcomments.rev_id = np.int64(scoredcomments.rev_id)\n",
    "\n",
    "print(\"scoredcomments.shape = \",scoredcomments.shape)\n",
    "scoredcomments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the text\n",
    "\n",
    "- Remove non alpha chars (numbers, etc)\n",
    "- Drop words less than 3 chars\n",
    "- Stem the words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 159686 samples:\n",
      "0 ,5000 ,10000 ,15000 ,20000 ,25000 ,30000 ,35000 ,40000 ,45000 ,50000 ,55000 ,60000 ,65000 ,70000 ,75000 ,80000 ,85000 ,90000 ,95000 ,100000 ,105000 ,110000 ,115000 ,120000 ,125000 ,130000 ,135000 ,140000 ,145000 ,150000 ,155000 ,\n",
      "\n",
      "stemmed_text:\n",
      " 0    this one can make analog mathemat term envis the distribut opinion popul gaussian curv would the...\n",
      "1    clarif for you and zundark right should have check the wikipedia bug page first this bug the cod...\n",
      "2                                                                                      elect elector jhk\n",
      "dtype: object\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "#stemmer = PorterStemmer() # alternate stemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# set up regex expression to remove all but alpha chars and whitespace\n",
    "regex = re.compile('[^a-zA-Z\\s]') \n",
    "\n",
    "numsamples = scoredcomments.comment.shape[0]\n",
    "\n",
    "# set minimum word size. Words with fewer characters are dropped. \n",
    "#  I do this because there are a lot of 2 char initials in the comment data, which I think aren't useful,\n",
    "#   ... or maybe they are - adjust this and see what happens!\n",
    "minwordsize = 3\n",
    "\n",
    "print(\"Processing %d samples:\"%(numsamples))\n",
    "\n",
    "# transform each sample text:\n",
    "stemmed_text = []\n",
    "for text,i in zip(scoredcomments.comment,range(numsamples)):\n",
    "    # set to lower case\n",
    "    text = regex.sub('',text.lower())\n",
    "    # look at each word in text\n",
    "    t = []\n",
    "    for word in word_tokenize(text):\n",
    "        # drop \"words\" that are too short or long (otherwise stem crashes!)\n",
    "        if len(word) >= minwordsize and len(word) < 30: \n",
    "            t.append(stemmer.stem(word)) # stem the added word\n",
    "    stemmed_text.append(\" \".join(t)) # re-combine list of stemmed words\n",
    "    if not i%5000: print(i,',', end=\"\")\n",
    "\n",
    "stemmed_text = pd.Series(np.array(stemmed_text)) # convert list of sample texts to pandas series\n",
    "    \n",
    "print(\"\\n\\nstemmed_text:\\n\",stemmed_text[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test a MultinomialNB classifier with text data\n",
    "\n",
    "I present two methods here that vary only in how the training and testing data are selected.\n",
    "\n",
    "- train_test_MultinomialNB uses all of the text data to train and test the classifier one time. This method is suitable for a first attempt to just get the thing working and then to do parameter tuning (which I don't do here). \n",
    "\n",
    "\n",
    "\n",
    "- cross_validate_MultinomialNB uses k-folds cross validation to partition the data into multiple non-overlapping train/test sets, and run the classifier on each. If the classifier is solid, the results should be the same for all sets. If the classifier has problems - for example it is overfitting, then you will see variation.\n",
    "\n",
    "Normally, you would spend a lot of time doing the first sort of training, tweaking, etc and then periodically use cross-validation as a \"reality check\" to verify that your model is robust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# helper function to report accuracy results of a prediction run\n",
    "def print_prediction_results(y_est, y_target):\n",
    "    \n",
    "    print(\"Classifier results:\")\n",
    "    \n",
    "    print(\"\\ttest set: #non-toxic = %d = %2.0f%%,  #toxic = %d = %2.0f%%\"%(\n",
    "        y_est[y_target==0].size, 100*y_est[y_target==0].size/y_est.size,\n",
    "        y_est[y_target==1].size, 100*y_est[y_target==1].size/y_est.size) )          \n",
    "\n",
    "    print(\"\\taccuracy all =    \\t%d/%d = %2.1f%%\"%(\n",
    "        (y_est == y_target).sum(), \n",
    "        y_est.size,\n",
    "        100*(y_est == y_target).sum() / y_est.size))\n",
    "\n",
    "    print(\"\\taccuracy non-toxic = \\t%d/%d = %2.1f%%\"%(\n",
    "        (y_est[y_target==0] == 0).sum(),\n",
    "        y_est[y_target==0].size,\n",
    "        100*(y_est[y_target==0] == 0).sum() / y_est[y_target==0].size))\n",
    "\n",
    "    print(\"\\taccuracy toxic = \\t%d/%d = %2.1f%%\"%(\n",
    "        (y_est[y_target==1] == 1).sum(), \n",
    "        y_est[y_target==1].size,\n",
    "        100*(y_est[y_target==1] == 1).sum() / y_est[y_target==1].size))\n",
    "    \n",
    "\n",
    "# train and test MultinomialNB with text string data X_text, category labels in y\n",
    "def train_test_MultinomialNB(X_text, y):\n",
    "    \n",
    "    # Tfidf vectorizer: vectorize the comment texts, and apply TF-IDF weighting\n",
    "    # Note that there are a bunch of parameter options, but I just use defaults here.\n",
    "    print(\"\\nVectorizing text data...\")\n",
    "    X_vectors = TfidfVectorizer().fit_transform(X_text)\n",
    "    print(\"X_vectors.shape = \",X_vectors.shape)\n",
    "\n",
    "    # create some training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_vectors, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # create and fit a niave baysian classifier to the training data\n",
    "    clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "    # generate predictions for test data\n",
    "    y_est = clf.predict(X_test)\n",
    "    \n",
    "    # print results of the prediction test\n",
    "    print_prediction_results(y_est, y_test)\n",
    "    \n",
    "\n",
    "# cross-validation of MultinomialNB with text string data X_text, category labels in y\n",
    "def cross_validate_MultinomialNB(X_text, y):\n",
    "\n",
    "    # Tfidf vectorizer: vectorize the comment texts, and apply TF-IDF weighting\n",
    "    # Note that there are a bunch of parameter options, but I just use defaults here.\n",
    "    print(\"\\nVectorizing text data...\")\n",
    "    X_vectors = TfidfVectorizer().fit_transform(X_text)\n",
    "    print(\"X_vectors.shape = \",X_vectors.shape)\n",
    "\n",
    "    # set up kfold to generate several train-test sets, \n",
    "    #  with shuffled indices for selecting from data\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    i = 1\n",
    "    accuracy = []\n",
    "    for train_index, test_index in kf.split(X_vectors, y):\n",
    "        print(\"\\nk-fold train/test set #%d: \"%(i))\n",
    "\n",
    "        # create and fit a niave baysian classifier to the training data\n",
    "        clf = MultinomialNB().fit(X_vectors[train_index,:], y[train_index])\n",
    "\n",
    "        # generate predictions for test data\n",
    "        y_est = clf.predict(X_vectors[test_index,:])\n",
    "\n",
    "        # print results of the prediction test\n",
    "        print_prediction_results(y_est, y[test_index])\n",
    "\n",
    "        accuracy.append((y_est == y[test_index]).sum() / y_est.size)\n",
    "        i += 1\n",
    "\n",
    "    print(\"\\nOverall accuracy = %2.1f%%\"%(np.mean(accuracy)*100))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, test and cross validate the classifier\n",
    "\n",
    "First, I look at results of training the classifier using all of the comment data, despite the fact that the number nontoxic comments is strongly over-represented (see counts of samples of each type in output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***************************\n",
      "Train and test classifier:\n",
      "\n",
      "Vectorizing text data...\n",
      "X_vectors.shape =  (159686, 171219)\n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 35346 = 89%,  #toxic = 4576 = 11%\n",
      "\taccuracy all =    \t36099/39922 = 90.4%\n",
      "\taccuracy non-toxic = \t35324/35346 = 99.9%\n",
      "\taccuracy toxic = \t775/4576 = 16.9%\n",
      "\n",
      "***************************\n",
      "Cross-validate classifier:\n",
      "\n",
      "Vectorizing text data...\n",
      "X_vectors.shape =  (159686, 171219)\n",
      "\n",
      "k-fold train/test set #1: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 28264 = 88%,  #toxic = 3674 = 12%\n",
      "\taccuracy all =    \t28911/31938 = 90.5%\n",
      "\taccuracy non-toxic = \t28243/28264 = 99.9%\n",
      "\taccuracy toxic = \t668/3674 = 18.2%\n",
      "\n",
      "k-fold train/test set #2: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 28264 = 88%,  #toxic = 3673 = 12%\n",
      "\taccuracy all =    \t28927/31937 = 90.6%\n",
      "\taccuracy non-toxic = \t28249/28264 = 99.9%\n",
      "\taccuracy toxic = \t678/3673 = 18.5%\n",
      "\n",
      "k-fold train/test set #3: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 28264 = 88%,  #toxic = 3673 = 12%\n",
      "\taccuracy all =    \t28920/31937 = 90.6%\n",
      "\taccuracy non-toxic = \t28255/28264 = 100.0%\n",
      "\taccuracy toxic = \t665/3673 = 18.1%\n",
      "\n",
      "k-fold train/test set #4: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 28264 = 88%,  #toxic = 3673 = 12%\n",
      "\taccuracy all =    \t28920/31937 = 90.6%\n",
      "\taccuracy non-toxic = \t28250/28264 = 100.0%\n",
      "\taccuracy toxic = \t670/3673 = 18.2%\n",
      "\n",
      "k-fold train/test set #5: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 28264 = 88%,  #toxic = 3673 = 12%\n",
      "\taccuracy all =    \t28875/31937 = 90.4%\n",
      "\taccuracy non-toxic = \t28241/28264 = 99.9%\n",
      "\taccuracy toxic = \t634/3673 = 17.3%\n",
      "\n",
      "Overall accuracy = 90.5%\n"
     ]
    }
   ],
   "source": [
    "# if you want, you can test the impact of my cleaning and stemming on results by passing the \n",
    "#   raw comments instead\n",
    "# text = scoredcomments.comment\n",
    "text = stemmed_text\n",
    "\n",
    "# train and test MultinomialNB classifier using all data\n",
    "print(\"\\n***************************\")\n",
    "print(\"Train and test classifier:\")\n",
    "train_test_MultinomialNB(text, scoredcomments.toxicity)\n",
    "    \n",
    "# cross-validate MultinomialNB classifier using nonoverlapping subsets of data\n",
    "print(\"\\n***************************\")\n",
    "print(\"Cross-validate classifier:\")\n",
    "cross_validate_MultinomialNB(text, scoredcomments.toxicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalize #samples of toxic vs non-toxic data\n",
    "\n",
    "\n",
    "In the previous result, you will notice that the classifier is super good at classifying non-toxic comments, but really sucks at classifying toxic comments. This is a problem if you want the classifier to detect toxic comments! Why does this happen? One possibility is that, since most of the training data are non-toxic, the classifier is being trained to  have a bias toward classifying most comments as non-toxic, because this gives the highest overall accuracy.\n",
    "\n",
    "So what if I equalize the number of non-toxic and toxic comments passed to the classifier for training?\n",
    "\n",
    "In the results below, look at the relative gain in accuracy at detecting toxic comments vs the loss of accuracy at detecting non-toxic comments - not a bad tradeoff!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data:\n",
      "#nontoxic =  141320\n",
      "#toxic =  18366\n",
      "\n",
      "***************************\n",
      "Train and test classifier:\n",
      "\n",
      "Vectorizing text data...\n",
      "X_vectors.shape =  (36732, 58715)\n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 4578 = 50%,  #toxic = 4605 = 50%\n",
      "\taccuracy all =    \t8063/9183 = 87.8%\n",
      "\taccuracy non-toxic = \t4215/4578 = 92.1%\n",
      "\taccuracy toxic = \t3848/4605 = 83.6%\n",
      "\n",
      "***************************\n",
      "Cross-validate classifier:\n",
      "\n",
      "Vectorizing text data...\n",
      "X_vectors.shape =  (36732, 58715)\n",
      "\n",
      "k-fold train/test set #1: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 3674 = 50%,  #toxic = 3674 = 50%\n",
      "\taccuracy all =    \t6450/7348 = 87.8%\n",
      "\taccuracy non-toxic = \t3364/3674 = 91.6%\n",
      "\taccuracy toxic = \t3086/3674 = 84.0%\n",
      "\n",
      "k-fold train/test set #2: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 3673 = 50%,  #toxic = 3673 = 50%\n",
      "\taccuracy all =    \t6451/7346 = 87.8%\n",
      "\taccuracy non-toxic = \t3356/3673 = 91.4%\n",
      "\taccuracy toxic = \t3095/3673 = 84.3%\n",
      "\n",
      "k-fold train/test set #3: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 3673 = 50%,  #toxic = 3673 = 50%\n",
      "\taccuracy all =    \t6415/7346 = 87.3%\n",
      "\taccuracy non-toxic = \t3373/3673 = 91.8%\n",
      "\taccuracy toxic = \t3042/3673 = 82.8%\n",
      "\n",
      "k-fold train/test set #4: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 3673 = 50%,  #toxic = 3673 = 50%\n",
      "\taccuracy all =    \t6427/7346 = 87.5%\n",
      "\taccuracy non-toxic = \t3331/3673 = 90.7%\n",
      "\taccuracy toxic = \t3096/3673 = 84.3%\n",
      "\n",
      "k-fold train/test set #5: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 3673 = 50%,  #toxic = 3673 = 50%\n",
      "\taccuracy all =    \t6416/7346 = 87.3%\n",
      "\taccuracy non-toxic = \t3357/3673 = 91.4%\n",
      "\taccuracy toxic = \t3059/3673 = 83.3%\n",
      "\n",
      "Overall accuracy = 87.6%\n"
     ]
    }
   ],
   "source": [
    "# number of samples to generate for each text category\n",
    "numtrainingsamples = np.sum(scoredcomments.toxicity==1)\n",
    "\n",
    "#text = commentdata.comment # use this to work with un-modified comment data\n",
    "text = np.array(stemmed_text) # use this to work with the cleaned and stemmed comment data\n",
    "\n",
    "# split the data by category.\n",
    "ind, = np.where(scoredcomments.toxicity==0)\n",
    "X_nontoxic = text[ind]\n",
    "target_nontoxic = scoredcomments.toxicity.values[ind]\n",
    "ind, = np.where(scoredcomments.toxicity==1)\n",
    "X_toxic = text[ind]\n",
    "target_toxic = scoredcomments.toxicity[ind]\n",
    "\n",
    "print(\"original data:\")\n",
    "print(\"#nontoxic = \",X_nontoxic.size)\n",
    "print(\"#toxic = \",X_toxic.size)\n",
    "\n",
    "# recombine the data with equalized number of samples of each category\n",
    "X_text = np.concatenate( (X_nontoxic[:numtrainingsamples],X_toxic), axis=0) \n",
    "target = np.concatenate( (target_nontoxic[:numtrainingsamples],target_toxic), axis=0) \n",
    "\n",
    "# train and test MultinomialNB classifier using all data\n",
    "print(\"\\n***************************\")\n",
    "print(\"Train and test classifier:\")\n",
    "train_test_MultinomialNB(X_text, target)\n",
    "    \n",
    "# cross-validate MultinomialNB classifier using nonoverlapping subsets of data\n",
    "print(\"\\n***************************\")\n",
    "print(\"Cross-validate classifier:\")\n",
    "cross_validate_MultinomialNB(X_text, target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
