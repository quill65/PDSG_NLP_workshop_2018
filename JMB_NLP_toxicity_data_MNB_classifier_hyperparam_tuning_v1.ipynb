{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter tuning of Multinomial Naive-Bayes classifier for toxic comment detector\n",
    "\n",
    "Created by John Burt\n",
    "\n",
    "This notebook is an example of how to tune parameters for a comment toxicity detector based on toxicity scored text data provided for the 2018 Portland Data Science Group NLP workshop meetup. The classifier model is a Multinomial Naive-Bayes classifier, and I use GridSearchCV to test the effect on accuracy of changes to various parameters.\n",
    "\n",
    "Setup:\n",
    "- Load comment and score data files, downloaded from http://dive-into.info/ \n",
    "- Combine comment and toxicity score data and generate toxicity categories (toxic vs non-toxic) for classifier training and prediction.\n",
    "\n",
    "Text pre-processing:\n",
    "- Clean up text by dropping non-alpha characters.\n",
    "- Drop words < 3 chars.\n",
    "- Use snowball stemmer to stem the words.\n",
    "\n",
    "Classifier method:\n",
    "- Use TfidfVectorizer to transform text into word count vectors and apply TDF/IDF algorithm to weight vectors.\n",
    "- MultinomialNB classifier using vectorized text data. \n",
    "\n",
    "Hyperparameter tuning:\n",
    "- Create a pipeline object containing the TfidfVectorizer and MultinomialNB stages.\n",
    "- Define a set of parameters and values to test.\n",
    "- Test the parameter values using GridSearchCV, which returns the best set of parameters.\n",
    "- Validate best model from GridSearchCV using kfolds cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Set up the notebook plot environment, import some basic modules, and load the data.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- This block also does a minimal bit of cleanup, by removing the embedded text \"NEWLINE_TOKEN\" and \"TAB_TOKEN\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments.shape =  (159686, 7)\n",
      "ratings.shape =  (1598289, 4)\n"
     ]
    }
   ],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "# set matplotlib environment and import some basics\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 100 # set to -1 to see entire text\n",
    "\n",
    "# ******************************************************\n",
    "# load the wikipedia toxicity data provided by Matt\n",
    "# ******************************************************\n",
    "# set True to load the smaller data set, False to load the large data set\n",
    "# NOTE: this code assumes data files are in the same folder as the notebook.\n",
    "if False:\n",
    "    # comment filename\n",
    "    commentfile = 'toxicity_annotated_comments_unanimous.tsv'\n",
    "    # rating filename\n",
    "    ratingfile = 'toxicity_annotations_unanimous.tsv'\n",
    "\n",
    "# full data set\n",
    "else:\n",
    "    # comment filename\n",
    "    commentfile = 'toxicity_annotated_comments.tsv'\n",
    "    # rating filename\n",
    "    ratingfile = 'toxicity_annotations.tsv'\n",
    "\n",
    "# load annotated comments\n",
    "comments = pd.read_table(commentfile)\n",
    "ratings = pd.read_table(ratingfile)\n",
    "\n",
    "# remove weird tab/newline TOKEN text\n",
    "comments['comment'] = comments['comment'].str.replace('NEWLINE_TOKEN','\\n')\n",
    "comments['comment'] = comments['comment'].str.replace('TAB_TOKEN','')\n",
    "\n",
    "# show shape of each data set\n",
    "print(\"comments.shape = \",comments.shape)\n",
    "print(\"ratings.shape = \",ratings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# uncomment this to download nltk corpus content, if you haven't done this already. \n",
    "#  This needs to be done once, and takes a while. \n",
    "#  I downloaded everything, but probably the \"popular packages\" will suffice.\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine comments and scores into one dataset\n",
    "\n",
    "Use Pandas groupby function to calculate the mean and median for each comment, and add them as columns to the comment dataframe. Now I have comments and two measures of score aligned.\n",
    "\n",
    "Next, I create a new toxicity categorical variable (0=not toxic, 1=toxic) by thresholding the median score at 0. I use the median score here because it is less sensitive to outlier scores than the mean.\n",
    "\n",
    "Note that I don't use the mean or median scores beyond this point - the Naive Bayes classifier wants a categorical variable. However, you could potentially do some other interesting things with these scores, including implement a different classifier that makes use of score data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoredcomments.shape =  (159686, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>median_score</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232</td>\n",
       "      <td>This:\\n:One can make an analogy in mathematical terms by envisioning the distribution of opinion...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216</td>\n",
       "      <td>`\\n\\n:Clarification for you  (and Zundark's right, i should have checked the Wikipedia bugs page...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953</td>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547</td>\n",
       "      <td>`This is such a fun entry.   Devotchka\\n\\nI once had a coworker from Korea and not only couldn't...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959</td>\n",
       "      <td>Please relate the ozone hole to increases in cancer, and provide figures. Otherwise, this articl...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id  \\\n",
       "0    2232   \n",
       "1    4216   \n",
       "2    8953   \n",
       "3   26547   \n",
       "4   28959   \n",
       "\n",
       "                                                                                               comment  \\\n",
       "0  This:\\n:One can make an analogy in mathematical terms by envisioning the distribution of opinion...   \n",
       "1  `\\n\\n:Clarification for you  (and Zundark's right, i should have checked the Wikipedia bugs page...   \n",
       "2                                                                            Elected or Electoral? JHK   \n",
       "3  `This is such a fun entry.   Devotchka\\n\\nI once had a coworker from Korea and not only couldn't...   \n",
       "4  Please relate the ozone hole to increases in cancer, and provide figures. Otherwise, this articl...   \n",
       "\n",
       "   year  logged_in       ns  sample  split  mean_score  median_score  toxicity  \n",
       "0  2002       True  article  random  train         0.4           0.5         0  \n",
       "1  2002       True     user  random  train         0.5           0.0         0  \n",
       "2  2002      False  article  random   test         0.1           0.0         0  \n",
       "3  2002       True  article  random  train         0.6           0.0         0  \n",
       "4  2002       True  article  random   test         0.2           0.0         0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoredcomments = comments.copy()\n",
    "# group all scores by comment ID for each text sample, add mean and median score columns to comment data \n",
    "scoredcomments[\"mean_score\"] = pd.Series(ratings.groupby(\"rev_id\",as_index=False).mean()[\"toxicity_score\"])\n",
    "scoredcomments[\"median_score\"] = pd.Series(ratings.groupby(\"rev_id\",as_index=False).median()[\"toxicity_score\"])\n",
    "\n",
    "# create catgorical variable toxicity: if median score < 0, toxicity=1, otherwise 0\n",
    "scoredcomments[\"toxicity\"] = (scoredcomments[\"median_score\"] < 0).astype(int)\n",
    "\n",
    "# make the comment id s ints\n",
    "scoredcomments.rev_id = np.int64(scoredcomments.rev_id)\n",
    "\n",
    "print(\"scoredcomments.shape = \",scoredcomments.shape)\n",
    "scoredcomments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Clean up the text\n",
    "\n",
    "- Remove non alpha chars (numbers, etc)\n",
    "- Drop words less than 3 chars\n",
    "- Stem the words with snowball stemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 159686 samples:\n",
      "0 ,5000 ,10000 ,15000 ,20000 ,25000 ,30000 ,35000 ,40000 ,45000 ,50000 ,55000 ,60000 ,65000 ,70000 ,75000 ,80000 ,85000 ,90000 ,95000 ,100000 ,105000 ,110000 ,115000 ,120000 ,125000 ,130000 ,135000 ,140000 ,145000 ,150000 ,155000 ,\n",
      "\n",
      "stemmed_text:\n",
      " 0    this one can make analog mathemat term envis the distribut opinion popul gaussian curv would the...\n",
      "1    clarif for you and zundark right should have check the wikipedia bug page first this bug the cod...\n",
      "2                                                                                      elect elector jhk\n",
      "dtype: object\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "#stemmer = PorterStemmer() # alternate stemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# set up regex expression to remove all but alpha chars and whitespace\n",
    "regex = re.compile('[^a-zA-Z\\s]') \n",
    "\n",
    "numsamples = scoredcomments.comment.shape[0]\n",
    "\n",
    "# set minimum word size. Words with fewer characters are dropped. \n",
    "#  I do this because there are a lot of 2 char initials in the comment data, which I think aren't useful,\n",
    "#   ... or maybe they are - adjust this and see what happens!\n",
    "minwordsize = 3\n",
    "\n",
    "print(\"Processing %d samples:\"%(numsamples))\n",
    "\n",
    "# transform each sample text:\n",
    "stemmed_text = []\n",
    "for text,i in zip(scoredcomments.comment,range(numsamples)):\n",
    "    # set to lower case\n",
    "    text = regex.sub('',text.lower())\n",
    "    # look at each word in text\n",
    "    t = []\n",
    "    for word in word_tokenize(text):\n",
    "        # drop \"words\" that are too short or long (otherwise stem crashes!)\n",
    "        if len(word) >= minwordsize and len(word) < 30: \n",
    "            t.append(stemmer.stem(word)) # stem the added word\n",
    "    stemmed_text.append(\" \".join(t)) # re-combine list of stemmed words\n",
    "    if not i%5000: print(i,',', end=\"\")\n",
    "\n",
    "stemmed_text = pd.Series(np.array(stemmed_text)) # convert list of sample texts to pandas series\n",
    "    \n",
    "print(\"\\n\\nstemmed_text:\\n\",stemmed_text[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalize the number of samples of toxic and non-toxic comments\n",
    "\n",
    "If you train multiNB classifier to all of the comment data, you will notice that it is very good at classifying non-toxic comments, but very poor at classifying toxic comments. That's because 89% of the training data are non-toxic and the classifier is being trained to have a bias toward labelling most comments as non-toxic, because this gives the highest overall accuracy.\n",
    "\n",
    "The accuracy at classifying toxic comments dramatically improves when you equalize the number of non-toxic and toxic comments passed to the classifier for training, with only a small drop in accuracy for non-toxic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data:\n",
      "#nontoxic =  141320\n",
      "#toxic =  18366\n"
     ]
    }
   ],
   "source": [
    "# number of samples to generate for each text category = # toxic comments\n",
    "numtrainingsamples = np.sum(scoredcomments.toxicity==1)\n",
    "\n",
    "#text = commentdata.comment # use this to work with un-modified comment data\n",
    "text = np.array(stemmed_text) # use this to work with the cleaned and stemmed comment data\n",
    "\n",
    "# split the data by category.\n",
    "ind, = np.where(scoredcomments.toxicity==0)\n",
    "X_nontoxic = text[ind]\n",
    "target_nontoxic = scoredcomments.toxicity.values[ind]\n",
    "ind, = np.where(scoredcomments.toxicity==1)\n",
    "X_toxic = text[ind]\n",
    "target_toxic = scoredcomments.toxicity[ind]\n",
    "\n",
    "print(\"original data:\")\n",
    "print(\"#nontoxic = \",X_nontoxic.size)\n",
    "print(\"#toxic = \",X_toxic.size)\n",
    "\n",
    "# recombine the data with equalized number of samples of each category\n",
    "X_text = np.concatenate( (X_nontoxic[:numtrainingsamples],X_toxic), axis=0) \n",
    "target = np.concatenate( (target_nontoxic[:numtrainingsamples],target_toxic), axis=0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the gridsearch \n",
    "\n",
    "The gridsearch using GridSearchCV is pretty straightforward: it takes a single estimator object, a set of parameters to test and some train/test data, and then exhaustively trains and tests the estimator with every parameter value combination to determine the one that gives the best score. [See here for more about hyperparameter tuning and gridsearch.](http://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "\n",
    "What is an estimator object? That is just an object that implements the scikit-learn estimator interface, which includes all scikit learn classifiers and regressors, as well as the various data transformation objects (i.e., Normalizer, Standardizer, etc). The classifier I use, MultinomialNB, is an estimator object and I could just pass that into gridsearch if I only wanted to tune the parameters available for MultinomialNB (which aren't many). But what if I want to also tune parameters for the data transformation steps before the classifier? For example, TfidfVectorizer has a lot of interesting parameters to tweak - how do I use GridSearchCV to find optimal parameters for both TfidfVectorizer and MultinomialNB? Enter the Pipeline object!\n",
    "\n",
    "The [Pipeline object](http://scikit-learn.org/stable/modules/pipeline.html#pipeline) is an estimator object that lets you chain multiple estimators so that you can transform data and and train a classifier in one step. You can chain as many estimators as you want and even use your own custom estimator objects. In this case, I'll pipeline the two estimators I've been using to classify the NLP data: TfidfVectorizer and MultinomialNB. Once I create the pipeline object containing these two estimators, I can then pass that to GridSearchCV and tune both at the same time.\n",
    "\n",
    "Method:\n",
    "\n",
    "- The general procedure is to run the gridsearch multiple times with different parameters and values to try and find the optimal parameter settings. You usually can't test all parameters and values in the same run (it could take a very long time to do everything at once), so you will be trying to test a few parameters at a time. \n",
    "\n",
    "\n",
    "- Define my own defaults. This step isn't necessary, but it's useful for setting parameters not being tuned to specific values that might not be the built-in defaults. For example I might find that removal of stop words is always better in TfidfVectorizer (not removing stop words is the normal default), in which case I set the stop_words param to \"english\".\n",
    "\n",
    "\n",
    "- Create the Pipeline object containing the estimator objects I want to test (TfidfVectorizer and MultinomialNB).\n",
    "\n",
    "\n",
    "- Create the parameter set to test. In this case, all of the parameters I want to test are with TfidfVectorizer, since MultinomialNB has very few. In this example, I provide all of the relevant parameters for TfidfVectorizer, but leave most of them commented out. This way I can try grid searches with different combos of parameters by simply leaving the ones I want uncommented. Try out some different parameters yourself ([see the documentation for TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for parameter details). Note the naming convention for gridsearch parameters: \"pipeline object name\"\\_\\_\"object parameter name\"\n",
    "\n",
    "\n",
    "- Create the GridSearchCV object, passing it the pipeline, the parameter set, and some train/test data. Note the parameter \"n_jobs=-1\": this tells GridSearchCV to start multiple processing threads to speed up the search, using all available processor cores on your computer (by default, it doesn't do this). Beware that your computer may get very busy while it is running gridsearch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Tfidf vectorizer:\n",
    "# define defaults: doing it this way allows us to define our own default params\n",
    "tfidfargs = {\n",
    "    \"analyzer\":'word', \n",
    "    \"max_features\" : None,\n",
    "    \"max_df\" : 0.25, # Filters out terms that occur in more than half of the docs (max_df=0.5)\n",
    "    \"min_df\" : 2, # Filters out terms that occur in only one document (min_df=2).\n",
    "    \"ngram_range\":(1, 3), # unigrams\n",
    "    \"stop_words\" : \"english\", # None, # \"english\", # Strips out “stop words”\n",
    "    \"use_idf\" : True\n",
    "    }\n",
    "\n",
    "# Define a pipeline combining a text vectorizer with a Naive Bayes classifier\n",
    "pipeline = Pipeline([    \n",
    "    ('tfidf', TfidfVectorizer(**tfidfargs)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Define the parameters and values we want to test.\n",
    "# Uncommenting more parameters will give better exploring power but will\n",
    "#   increase processing time in a combinatorial way. I suggest tuning <= 3\n",
    "#   parameters at a time.\n",
    "# Note the naming format: pipelineobjectname__paramname\n",
    "parameters = {\n",
    "    'tfidf__stop_words': ('english', None),\n",
    "    #'tfidf__analyzer': ('word', 'char_wb'),\n",
    "    #'tfidf__analyzer': ('word', 'char', 'char_wb'),\n",
    "    #'tfidf__max_df': (0.1, 0.25, 0.5, 0.75),\n",
    "    #'tfidf__min_df': (1,2,5),\n",
    "    #'tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2), (2, 2), (1, 3), (3, 3)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "}\n",
    "\n",
    "# create grid search object to find the best parameters for both the \n",
    "#   feature extraction and the classifier.\n",
    "# Note: n_jobs=-1 causes GridSearchCV to use multithreading to employ all processor cores.\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the grid search\n",
    "\n",
    "The gridsearch takes the pipeline object (containing the text vectorizer and the multiNB classifier) and the data and tries all combos of the parameters we have defined. The output parameter \"best\\_estimator\\_\" contains a pipeline object with the parameters that give best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'tfidf__stop_words': ('english', None), 'tfidf__ngram_range': ((1, 1), (1, 2), (2, 2), (1, 3), (3, 3))}\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 68.492s\n",
      "\n",
      "Best score: 0.865\n",
      "Best parameters set:\n",
      "\ttfidf__ngram_range: (1, 1)\n",
      "\ttfidf__stop_words: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_text, target)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the best classifier using k-folds cross validation\n",
    "\n",
    "\n",
    "\n",
    "cross_validate_classifier uses k-folds cross validation to partition the data into multiple non-overlapping train/test sets, and run the classifier on each. If the classifier is solid, the results should be the same for all sets. If the classifier has problems - for example it is overfitting, then you will see variation.\n",
    "\n",
    "Normally, you would spend a lot of time doing the first sort of training, tweaking, etc and then periodically use cross-validation as a \"reality check\" to verify that your model is robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# helper function to report accuracy results of a prediction run\n",
    "def print_prediction_results(y_est, y_target):\n",
    "    \n",
    "    print(\"Classifier results:\")\n",
    "    \n",
    "    print(\"\\ttest set: #non-toxic = %d = %2.0f%%,  #toxic = %d = %2.0f%%\"%(\n",
    "        y_est[y_target==0].size, 100*y_est[y_target==0].size/y_est.size,\n",
    "        y_est[y_target==1].size, 100*y_est[y_target==1].size/y_est.size) )          \n",
    "\n",
    "    print(\"\\taccuracy all =    \\t%d/%d = %2.1f%%\"%(\n",
    "        (y_est == y_target).sum(), \n",
    "        y_est.size,\n",
    "        100*(y_est == y_target).sum() / y_est.size))\n",
    "\n",
    "    print(\"\\taccuracy non-toxic = \\t%d/%d = %2.1f%%\"%(\n",
    "        (y_est[y_target==0] == 0).sum(),\n",
    "        y_est[y_target==0].size,\n",
    "        100*(y_est[y_target==0] == 0).sum() / y_est[y_target==0].size))\n",
    "\n",
    "    print(\"\\taccuracy toxic = \\t%d/%d = %2.1f%%\"%(\n",
    "        (y_est[y_target==1] == 1).sum(), \n",
    "        y_est[y_target==1].size,\n",
    "        100*(y_est[y_target==1] == 1).sum() / y_est[y_target==1].size))\n",
    "    \n",
    "\n",
    "# cross-validation of classifier model with text string data X_text, category labels in y\n",
    "def cross_validate_classifier(clf, X_text, y):\n",
    "\n",
    "    # set up kfold to generate several train-test sets, \n",
    "    #  with shuffled indices for selecting from data\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    i = 1\n",
    "    accuracy = []\n",
    "    for train_index, test_index in kf.split(X_text, y):\n",
    "        print(\"\\nk-fold train/test set #%d: \"%(i))\n",
    "\n",
    "        # fit the classifier with training data\n",
    "        clf.fit(X_text[train_index], y[train_index])\n",
    "\n",
    "        # generate predictions for test data\n",
    "        y_est = clf.predict(X_text[test_index])\n",
    "\n",
    "        # print results of the prediction test\n",
    "        print_prediction_results(y_est, y[test_index])\n",
    "\n",
    "        accuracy.append((y_est == y[test_index]).sum() / y_est.size)\n",
    "        i += 1\n",
    "\n",
    "    print(\"\\nOverall accuracy = %2.1f%%\"%(np.mean(accuracy)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***************************\n",
      "Cross-validate classifier:\n",
      "\n",
      "k-fold train/test set #1: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 3674 = 50%,  #toxic = 3674 = 50%\n",
      "\taccuracy all =    \t6445/7348 = 87.7%\n",
      "\taccuracy non-toxic = \t3282/3674 = 89.3%\n",
      "\taccuracy toxic = \t3163/3674 = 86.1%\n",
      "\n",
      "k-fold train/test set #2: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 3673 = 50%,  #toxic = 3673 = 50%\n",
      "\taccuracy all =    \t6467/7346 = 88.0%\n",
      "\taccuracy non-toxic = \t3308/3673 = 90.1%\n",
      "\taccuracy toxic = \t3159/3673 = 86.0%\n",
      "\n",
      "k-fold train/test set #3: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 3673 = 50%,  #toxic = 3673 = 50%\n",
      "\taccuracy all =    \t6446/7346 = 87.7%\n",
      "\taccuracy non-toxic = \t3279/3673 = 89.3%\n",
      "\taccuracy toxic = \t3167/3673 = 86.2%\n",
      "\n",
      "k-fold train/test set #4: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 3673 = 50%,  #toxic = 3673 = 50%\n",
      "\taccuracy all =    \t6420/7346 = 87.4%\n",
      "\taccuracy non-toxic = \t3292/3673 = 89.6%\n",
      "\taccuracy toxic = \t3128/3673 = 85.2%\n",
      "\n",
      "k-fold train/test set #5: \n",
      "Classifier results:\n",
      "\ttest set: #non-toxic = 3673 = 50%,  #toxic = 3673 = 50%\n",
      "\taccuracy all =    \t6479/7346 = 88.2%\n",
      "\taccuracy non-toxic = \t3334/3673 = 90.8%\n",
      "\taccuracy toxic = \t3145/3673 = 85.6%\n",
      "\n",
      "Overall accuracy = 87.8%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cross-validate MultinomialNB classifier using nonoverlapping subsets of data\n",
    "print(\"\\n***************************\")\n",
    "print(\"Cross-validate classifier:\")\n",
    "cross_validate_classifier(grid_search.best_estimator_, X_text, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
